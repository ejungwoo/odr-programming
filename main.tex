\documentclass[chapter,a4paper,12pt]{oblivoir}

\usepackage[margin=24mm]{geometry}
\usepackage{graphicx}
\usepackage{relsize}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{url}

\begin{document}

\title{회기분석(Regression Analysis)\\ 이게 회기분석이 맞는지는 잘 모름}
\author{이정우\\phyjics@gmail.com}
\maketitle
\tableofcontents

\newpage
\chapter{최소 제곱법(Least Squares Fitting)}

\section{선형 회기 - 직선}

N개의 데이터 $(x_i, y_i)$가 직선
\begin{equation}
f(x) = ax + b,
\end{equation}
의 분포를 따른다고 할 때 최소 제곱법은 데이터와 직선 사이의 $y$값 차이를 제곱하여 합한 값을 최소화 하는 방법이다.
최소화 하고자 하는 값 $S$는 다음과 같다.
\begin{equation}
S = \sum_{i=1}^N (y_i - ax_i - b)^2.
\end{equation}
이를 변수 $a$와 $b$에 대해서 각각 최소화 하면;
\begin{equation}
\frac{\partial S}{\partial a} = 0,\,\,\,\, \frac{\partial S}{\partial b} = 0,
\end{equation}
아래와 같이 $a$와 $b$를 구할 수 있다.
\begin{eqnarray}
a &=& \frac{N\sum x_i y_i - \sum x_i \sum y_i}{N\sum x_i^2 - (\sum x_i)^2}.\\\nonumber\\
b &=& \frac{\sum x_i^2 \sum y_i - \sum x_i \sum x_i y_i}{N\sum x_i^2 - (\sum x_i)^2}.
\end{eqnarray}

%\begin{eqnarray}
%\sigma_y^2 &=& \sum_{i=1}^N P_i (y_i - ax_i - b)^2\\
%           &=& \frac{1}{N}\sum_{i=1}^N (y_i - ax_i - b)^2
%\end{eqnarray}

\newpage
\section{함수를 알고 있는 경우}

N개의 데이터 $(x_i, y_i)$가
\begin{equation}
f(x) = av(x-b),
\end{equation}
의 분포를 따른다고 할 때 최소화 하고자 하는 $S$는
\begin{equation}
S = \sum_{i=1}^N \left(y_i - av(x_i - b)\right)^2,
\end{equation}
이 된다. 이를 $a$와 $b$에 대해서 최소화 하면 
(${\partial S}/{\partial a} = 0,\,\,{\partial S}/{\partial b} = 0$) 
다음과 같다.
\begin{eqnarray}
\frac{\partial S}{\partial a} &\longrightarrow& a = \frac{\sum y_i v(x_i - b)}{\sum f^2(x_i - b)}.%,\\\nonumber\\
\end{eqnarray}
함수의 형태를 알지 못할 떄 변수 $b$는 함수 $v$에 종속되어 있어 구할 수 없다. 하지만 특정 $b$에서 최소 제곱법에 맞는 $a$를 구할 수 있다.
함수의 형태를 알고 있을 때는 경우에 따라 다음 식을 이용하여 $b$를 구할 수 있다.
\begin{eqnarray}
\frac{\partial s}{\partial b} &\longrightarrow& a = \frac{\sum y_i v'(x_i - b)}{\sum v(x_i - b)v'(x_i - b)}\\\nonumber\\
&&v'(x) \equiv \frac{\partial v(x)}{\partial x}
\end{eqnarray}

\chapter{수직 거리 회기(Orthogonal Distance Regression (ODR))를 이용한 최소 제곱법}
\section{평면 피팅}\label{section_fitplane}

3차원에서 주어진 N개의 점 데이터를 이용하여 가장 잘 맞는 평면을 피팅해보자.
이 문서에서 다룰 평면 피팅은 ODR(수직 거리 회기, Orthogonal Distance Regression) 피팅으로
각 점에서 평면까지 수직 거리 제곱의 합을 최소화 하는 방법이다.
일반적인 피팅 방법에서는 문제에서 최소화 할 $S$를 정한 뒤 변수를 바꿔가면서
$S$가 최소가 되는 점을 찾아 나가는 반복 계산을 한다.
하지만 ODR 피팅은 $S$가 최소가 되는 지점을 3차원 행렬의 고윳값 문제로 해결 할 수 있다는 것이 증명되어 있다.
또한 이 방법으로 평면 피팅 뿐만 아니라 선 피팅도 할 수 있다.

N개의 점 데이터를 평면($ax+by+cz=d$)으로 피팅을 하고 싶다. $i$번째 점의 가중치는 $w_i$이며 위치는 ($x_i, y_i, z_i$)로 주어진다.
평면과 $i$번째 점 사이의 수직 거리는 다음과 같다.
\begin{equation}
D_i(a,b,c,d) = \frac{\vert ax_i + by_i + cz_i + d \vert}{\sqrt{a^2 + b^2 + c^2}}.
\end{equation}
N개의 점에 대해서 가중치를 고려한 거리 제곱 $D_i^2$의 합은 다음과 같다.
\begin{eqnarray} \label{S}
S(a,b,c,d) &=& \sum_{i=1}^N w_i\, D_i(a,b,c,d)^2 \\\nonumber
&=& \sum_{i=1}^N w_i\, \frac{\vert ax_i + by_i + cz_i + d \vert^2}{a^2 + b^2 + c^2}.
\end{eqnarray}
$S$를 변수 $d$에 대해서 최소화 하면 ${\partial S}/{\partial d} = 0$ 이므로 다음을 얻는다.
\begin{equation} \label{minimize_d}
d = -\Big(a\langle x \rangle_N + b\langle y \rangle_N + c\langle z \rangle_N\Big).
\end{equation}
여기서 $\langle x \rangle_N, \langle y \rangle_N, \langle z \rangle_N$는 N개 데이터에 대한 기대값이고
\begin{eqnarray} \label{expectation_value}
\langle x \rangle_N \equiv \frac{1}{W_N}\sum_{i=1}^N w_i\,x_i\\
\langle y \rangle_N \equiv \frac{1}{W_N}\sum_{i=1}^N w_i\,y_i\\
\langle z \rangle_N \equiv \frac{1}{W_N}\sum_{i=1}^N w_i\,z_i
\end{eqnarray}
%\end{array}\right.
%\end{equation}
$W$는 가중치의 합이다.
\begin{equation} \label{W}
W_N = \sum_{i=1}^N w_i. \\
\end{equation}
식 (\ref{minimize_d})을 평면의 방정식에 대입하면 다음과 같다.
\begin{equation}
a\big(x_i-\langle x \rangle_N\big) + b\big(y_i-\langle y \rangle_N\big) + c\big(z_i-\langle z \rangle_N\big) = 0.
\end{equation}
즉, 우리가 구하려고 하는 평면은 반드시 데이터의 중심점을 지나간다는 사실을 알 수 있다.
나아가 식 (\ref{S})는
\begin{eqnarray} \label{S2}
S(a,b,c) 
&=& \sum_{i=1}^N w_i\, \frac{\vert ax_i + by_i + cz_i - a\langle x \rangle_N - b\langle y \rangle_N - c\langle z \rangle_N \vert^2}{a^2 + b^2 + c^2} \\
&=& \sum_{i=1}^N \frac{\vert a \sqrt{w_i}(x_i-\langle x \rangle_N) + b \sqrt{w_i}(y_i-\langle y \rangle_N) + c \sqrt{w_i}(z_i-\langle z \rangle_N) \vert^2}{a^2 + b^2 + c^2} \nonumber\\
&=& \sum_{i=1}^N \frac{\vert aX_{N,i} + bY_{N,i} + cZ_{N,i} \vert^2}{a^2 + b^2 + c^2} \nonumber
\end{eqnarray}
이 된다. $X_{N,i}$, $Y_{N,i}$, $Z_{N,i}$는 다음과 같이 정의한다.
\begin{eqnarray}
X_{N,i} \equiv \sqrt{w_i}\,(x_i - \langle x \rangle_N)\\
Y_{N,i} \equiv \sqrt{w_i}\,(y_i - \langle y \rangle_N)\\
Z_{N,i} \equiv \sqrt{w_i}\,(z_i - \langle z \rangle_N)
\end{eqnarray}
%\begin{equation}
%\left.
%\begin{array}{ll}
%X_i \equiv \sqrt{w_i} (x_i - \langle x \rangle) \\
%Y_i \equiv \sqrt{w_i} (y_i - \langle y \rangle) \\
%Z_i \equiv \sqrt{w_i} (z_i - \langle z \rangle) 
%\end{array}\right.
%\end{equation}
이제 $S$를 행렬식으로 쓰면
\begin{eqnarray}
\mathbf{v^T} &\equiv& (a,b,c,), \\\nonumber\\
\mathbf{M}   &\equiv& \left( \begin{array}{ccc}
  X_1 & Y_1 & Z_1 \\
  X_2 & Y_2 & Z_2 \\
  \vdots & \vdots & \vdots \\
  X_N & Y_N & Z_N 
\end{array} \right),
\end{eqnarray}
일때 다음과 같이 다시 쓸 수 있다.
\begin{equation}
S(\mathbf v) = \frac{\mathbf{v^T}\mathbf{M^T}\mathbf{M}\mathbf{v}}{\mathbf{v^T}\mathbf{v}}
             = \frac{\mathbf{v^T}\mathbf{A}\mathbf{v}}{\mathbf{v^T}\mathbf{v}}.
\end{equation}
\begin{equation}
\mathbf{A}\equiv\mathbf{M^T}\mathbf{M}
\end{equation}
$S(\mathbf{v})$는 Rayleigh Quotient로 행렬 $\mathbf{A}$의 
가장 작은 고윳값(eigen value)을 택하였을때 최소화 되며, 반대로 가장 큰 고윳값을 택하였을때 최대화 된다.
이때 고유벡터(eigen vector) $\mathbf v$는 평면의 수직 벡터가 된다.

행렬 $\mathbf{M}$에 대해서 특이값 분해(Singular value decomposition)를 하고 이를 행렬$\mathbf{A}$에
대입하면
\begin{eqnarray}
\mathbf{M}&=&\mathbf{U}\mathbf{S}\mathbf{V^T} \\
\mathbf{A}&=&\mathbf{V}\mathbf{S^2}\mathbf{V^T} \label{def_A}
\end{eqnarray}
가 된다. 즉 행렬 $\mathbf{A}$의 고윳값은 행렬 $\mathbf{M}$의 특이값의 제곱이 되며 
행렬 $\mathbf{A}$의 고유벡터는 행렬 $\mathbf{M}$의 특이벡터와 같다.
여기서 특이값을 구하거나 고윳값을 구하므로써 평면을 구할 수 있는데 아직 특이값에 대해 자세히 알지 못하므로
어느 방법이 더 좋은지 알 수 없다(추가 바람).
따라서 지금은 행렬 $\mathbf{A}$의 고윳값 문제로 해결하도록 한다.

식 (\ref{def_A})에 따라 행렬 $\mathbf{A}$를 N의 함수로 풀어 쓰면 다음과 같다.
\begin{equation} \label{matrix_A}
\mathbf{A}_N = \sum_{i=1}^N\left( \begin{array}{ccc}
  X_{N,i}^2 & X_{N,i}\,Y_{N,i} & X_{N,i}\,Z_{N,i} \\
  Y_{N,i}\,X_{N,i} & Y_{N,i}^2 & Y_{N,i}\,Z_{N,i} \\
  Z_{N,i}\,X_{N,i} & Z_{N,i}\,Y_{N,i} & Z_{N,i}^2
\end{array} \right).
\end{equation}
이 행렬은 $W_N$로 나누었을때 공분산 행렬(covariance matrix)이 된다.

\section{가만있어봐라}
행렬 $\mathbf{A}$의 성분을 정리하면 다음과 같다.
\begin{eqnarray}
\left(\mathbf{A}_{N}\right)_{11} &=& \sum_{i=1}^{N} X_{N,i}^2 \\
&=& \sum_{i=1}^{N} w_i\,(x_i - \langle x \rangle_{N})^2\nonumber\\
&=& \sum_{i=1}^{N} w_i\,(x_i^2 -2x_i\langle x \rangle_{N} + \langle x \rangle_{N}^2) \nonumber\\
&=& \sum_{i=1}^{N} w_i\,x_i^2 -2\langle x \rangle_{N}\sum_{i=1}^{N}w_i\,x_i + \langle x \rangle_{N}^2\sum_{i=1}^{N}w_i \nonumber\\
&=& W_{N}\,\langle x^2 \rangle_{N} -2W_{N}\,\langle x \rangle_{N}^2 + W_{N}\,\langle x \rangle_{N}^2 \nonumber\\
&=& W_{N}\Big(\langle x^2 \rangle_{N} -\langle x \rangle_{N}^2\Big). \nonumber
\end{eqnarray}
\begin{eqnarray}
\left(\mathbf{A}_{N}\right)_{12} &=& \sum_{i=1}^{N} X_iY_i \\
&=& \sum_{i=1}^{N} w_i\,(x_i - \langle x \rangle_{N})(y_i - \langle y \rangle_{N})\nonumber\\
&=& \sum_{i=1}^{N} w_i\,(x_i\,y_i + \langle y \rangle_{N}\langle x \rangle_{N} - x_i\langle y \rangle_{N} - y_i\langle x \rangle_{N} )\nonumber\\
&=& \sum_{i=1}^{N} w_i\,x_i\,y_i + \langle x \rangle_{N}\,\langle y \rangle_{N} \sum_{i=1}^{N} w_i - \langle y \rangle_{N} \sum_{i=1}^{N} w_i\,x_i - \langle x \rangle_{N} \sum_{i=1}^{N} w_i\,y_i \nonumber\\
&=& W_{N}\langle xy \rangle_{N}  + W_{N}\langle x \rangle_{N}\,\langle y \rangle_{N} - W_{N}\langle y \rangle_{N} \langle x \rangle_{N} - W_{N}\langle x \rangle_{N} \langle y \rangle_{N}\nonumber\\
&=& W_{N} \Big(\langle xy \rangle_{N} + \langle x \rangle_{N}\langle y \rangle_{N} - \langle x \rangle_{N} - \langle y \rangle_{N} \Big). \nonumber
\end{eqnarray}
행렬 $\mathbf{A}$는 대칭 행렬이며 각 행에 따라서 $x, y, z$의 순서만 바뀌기 때문에 위 두 성분을 이용하여 다른 성분들 쉽게 알 수 있다.

이제 기존 데이터에서 다른 데이터를 추가할 때 행렬 $\mathbf{A}$가 어떻게 변하는지 알아보자. 각 성분들은 다음과 같다.
\begin{eqnarray}
\left(\mathbf{A}_{N+1}\right)_{11} &=& W_{N+1}\Big(\langle x^2 \rangle_{N+1} -\langle x \rangle_{N+1}^2\Big), \\
\left(\mathbf{A}_{N+1}\right)_{22} &=& W_{N+1}\Big(\langle y^2 \rangle_{N+1} -\langle y \rangle_{N+1}^2\Big), \\
\left(\mathbf{A}_{N+1}\right)_{33} &=& W_{N+1}\Big(\langle z^2 \rangle_{N+1} -\langle z \rangle_{N+1}^2\Big), \\
\left(\mathbf{A}_{N+1}\right)_{12} &=& W_{N+1} \Big(\langle xy \rangle_{N+1} + \langle x \rangle_{N+1}\langle y \rangle_{N+1} - \langle x \rangle_{N+1} - \langle y \rangle_{N+1} \Big), \\
\left(\mathbf{A}_{N+1}\right)_{13} &=& W_{N+1} \Big(\langle xz \rangle_{N+1} + \langle x \rangle_{N+1}\langle z \rangle_{N+1} - \langle x \rangle_{N+1} - \langle z \rangle_{N+1} \Big), \\
\left(\mathbf{A}_{N+1}\right)_{23} &=& W_{N+1} \Big(\langle yz \rangle_{N+1} + \langle y \rangle_{N+1}\langle z \rangle_{N+1} - \langle y \rangle_{N+1} - \langle z \rangle_{N+1} \Big). 
\end{eqnarray}
각 성분들을 이루는 기대값들은 다음과 같이 기존 기대값을 이용하여 구할 수 있다.
\begin{eqnarray}
\langle x \rangle_{N+1} &=& \frac{1}{W_{N+1}}\sum_{i=1}^{N+1}w_i\,x_i \\
&=& \frac{1}{W_{N+1}}\left(\sum_{i=1}^{N}w_i\,x_i + w_{N+1}\,x_{N+1}\right)\nonumber\\
&=& \frac{1}{W_{N+1}}\,\Big(W_N\langle x \rangle_N + w_{N+1}\,x_{N+1}\Big).\nonumber
\end{eqnarray}
\begin{eqnarray}
\langle x^2 \rangle_{N+1} &=& \frac{1}{W_{N+1}}\sum_{i=1}^{N+1}w_i\,x_i^2 \\
&=& \frac{1}{W_{N+1}}\left(\sum_{i=1}^{N}w_i\,x_i^2 + w_{N+1}\,x_{N+1}^2\right)\nonumber\\
&=& \frac{1}{W_{N+1}}\,\Big(W_N\langle x^2 \rangle_N + w_{N+1}\,x_{N+1}^2\Big).\nonumber
\end{eqnarray}
\begin{eqnarray}
\langle xy \rangle_{N+1} &=& \frac{1}{W_{N+1}}\sum_{i=1}^{N+1}w_i\,x_i\,y_i \\
&=& \frac{1}{W_{N+1}}\left(\sum_{i=1}^{N}w_i\,x_i\,y_i + w_{N+1}\,x_{N+1}\,y_{N+1}\right)\nonumber\\
&=& \frac{1}{W_{N+1}}\,\Big(W_N\langle xy \rangle_N + w_{N+1}\,x_{N+1}\,y_{N+1}\Big).\nonumber
\end{eqnarray}

$k$ 번째 데이터를 하나 제거 하는 경우는 다음과 같다.
\begin{eqnarray}
\left(\mathbf{A}_{N,!k}\right)_{11} &=& W_{N,!k}\Big(\langle x^2 \rangle_{N,!k} -\langle x \rangle_{N,!k}^2\Big), \\
\left(\mathbf{A}_{N,!k}\right)_{22} &=& W_{N,!k}\Big(\langle y^2 \rangle_{N,!k} -\langle y \rangle_{N,!k}^2\Big), \\
\left(\mathbf{A}_{N,!k}\right)_{33} &=& W_{N,!k}\Big(\langle z^2 \rangle_{N,!k} -\langle z \rangle_{N,!k}^2\Big), \\
\left(\mathbf{A}_{N,!k}\right)_{12} &=& W_{N,!k} \Big(\langle xy \rangle_{N,!k} + \langle x \rangle_{N,!k}\langle y \rangle_{N,!k} - \langle x \rangle_{N,!k} - \langle y \rangle_{N,!k} \Big), \\
\left(\mathbf{A}_{N,!k}\right)_{13} &=& W_{N,!k} \Big(\langle xz \rangle_{N,!k} + \langle x \rangle_{N,!k}\langle z \rangle_{N,!k} - \langle x \rangle_{N,!k} - \langle z \rangle_{N,!k} \Big), \\
\left(\mathbf{A}_{N,!k}\right)_{23} &=& W_{N,!k} \Big(\langle yz \rangle_{N,!k} + \langle y \rangle_{N,!k}\langle z \rangle_{N,!k} - \langle y \rangle_{N,!k} - \langle z \rangle_{N,!k} \Big). 
\end{eqnarray}
위에서와 같이 기대값은 아래와 같이 구할 수 있다.
\begin{eqnarray}
\langle x \rangle_{N,!k} &=& \frac{1}{W_{N,!k}}\sum_{i=1, i\neq k}^{N}w_i\,x_i \\
&=& \frac{1}{W_{N,!k}}\left(\sum_{i=1}^{N}w_i\,x_i - w_{k}\,x_{k}\right)\nonumber\\
&=& \frac{1}{W_{N,!k}}\,\Big(W_N\langle x \rangle_N - w_{k}\,x_{k}\Big).\nonumber
\end{eqnarray}
\begin{eqnarray}
\langle x^2 \rangle_{N,!k} &=& \frac{1}{W_{N,!k}}\sum_{i=1, i\neq k}^{N}w_i\,x_i^2 \\
&=& \frac{1}{W_{N,!k}}\left(\sum_{i=1}^{N}w_i\,x_i^2 - w_{k}\,x_{k}^2\right)\nonumber\\
&=& \frac{1}{W_{N,!k}}\,\Big(W_N\langle x^2 \rangle_N - w_{k}\,x_{k}^2\Big).\nonumber
\end{eqnarray}
\begin{eqnarray}
\langle xy \rangle_{N,!k} &=& \frac{1}{W_{N,!k}}\sum_{i=1, i\neq k}^{N}w_i\,x_i\,y_i \\
&=& \frac{1}{W_{N,!k}}\left(\sum_{i=1}^{N}w_i\,x_i\,y_i - w_{k}\,x_{k}\,y_{k}\right)\nonumber\\
&=& \frac{1}{W_{N,!k}}\,\Big(W_N\langle xy \rangle_N - w_{k}\,x_{k}\,y_{k}\Big).\nonumber
\end{eqnarray}

\section{직선 피팅}
점($x_0, y_0, z_0$)을 지나가고 ($a,b,c$)의 방향 벡터를 가지는 직선
\begin{equation}
\left.
\begin{array}{ll}
x = x_0 + at \\\nonumber
y = y_0 + bt \\\nonumber
z = z_0 + ct
\end{array}\right.
\end{equation}
과 점($x_i, y_i, z_i$) 사이의 수직 거리는 다음과 같다.
\begin{equation}
D_i(x_0,y_0,z_0,a,b,c) = \left( 
\begin{array}{ll}
  &\left(c(y_i-y_0) - b(z_i - z_0)\right)^2 \\
  + &\left(a(z_i-z_0) - c(x_i - x_0)\right)^2 \\
  + &\left(b(x_i-x_0) - a(y_i - y_0)\right)^2 
\end{array} \right)^{1/2}
\end{equation}
우리는 점과 직선사이 거리 제곱의 합을 최소화 할 것이므로 $S$는 다음과 같이 쓸 수 있다.
\begin{equation}
S(x_0,y_0,z_0,a,b,c) = \mathlarger w_i \sum_{i=1}^N\left( 
\begin{array}{ll}
  &\left(c(y_i-y_0) - b(z_i - z_0)\right)^2 \\
  + &\left(a(z_i-z_0) - c(x_i - x_0)\right)^2 \\
  + &\left(b(x_i-x_0) - a(y_i - y_0)\right)^2 
\end{array} \right)
\end{equation}
이제 $S$를 $x_0$, $y_0$, $z_0$에 대해서 각각 최소화 하면
(${\partial S}/{\partial x_0}=0$, ${\partial S}/{\partial y_0}=0$, ${\partial S}/{\partial z_0}=0$)
다음식을 얻을 수 있다.
\begin{equation}
\frac{x_0-\langle x \rangle_N}{a} = \frac{y_0-\langle y \rangle_N}{b} = \frac{z_0-\langle z \rangle_N}{c}
\end{equation}
여기서 $\langle x \rangle_N , \langle y \rangle_N, \langle z \rangle_N$는 식 (\ref{expectation_value})와 같다.
위 식은 ($x_0,y_0,z_0$)가 ($\langle x \rangle_N,\langle y \rangle_N,\langle z \rangle_N$)일때 만족한다. 
따라서 우리가 구하고자 하는 직선은 데이터의 중심점을 지나간다.

데이터 점을 $R_i$, 데이터의 중심점을 $C$, 우리가 구하고자 하는 직선을 $L$, 
그 직선에 수직하고 점 $C$를 포함하는 평면을 $P$, 그리고 
$D(\alpha, \beta)$를 $\alpha$와 $\beta$ 사이의 거리라고 할때, 
피타고라스의 정리를 이용하면 다음과 같이 쓸 수 있다.
\begin{equation}
\sum_{i=1}^N D(R_i, L)^2 = \sum_{i=1}^N D(R_i, C)^2 - \sum_{i=1}^N D(R_i, P)^2
\end{equation}
위의 $\sum D(R_i, C)^2$는 상수임을 알 수 있다.
즉 우리가 최소화 하고자 하는 $S(=\sum D(R_i, L)^2)$는 
$\sum D(R_i, P)^2$를 최대화 하므로써 알 수 구할 수 있다.
그런데 $\sum D(R_i, P)^2$는 \ref{section_fitplane}절의 $S$(식 (\ref{S}), (\ref{S2}))와 같으므로
행렬 $\mathbf{A}$(식 (\ref{matrix_A}))의 가장 큰 고윳값을 찾는 문제로 귀결된다.


%\section{워어어어어어어어어언}
%원
%\begin{equation}
%(x-a)^2 + (y-b)^2 = r^2,
%\end{equation}
%과 점 사이의 거리는
%\begin{equation}
%D_i(x_0,y_0,z_0,a,b,r) = 
%r - \sqrt{(x_0-a)^2 - (y_0-b)^2}.
%\end{equation}
%$s_C$는 다음과 같이 쓸 수 있다.
%\begin{equation}
%s_C(x_0,y_0,z_0,a,b,r) = %\mathlarger
%\sum_{i=1}^N\left( r - \sqrt{(x_0-a)^2 - (y_0-b)^2} \right)^2
%\end{equation}



\begin{thebibliography}{}

\bibitem{bestfitplane}
The Math Forum - Line of Best Fit For Points in Three Dimensional Space
(\url{http://mathforum.org/library/drmath/view/69103.html}).

\bibitem{bestfitline}
The Math Forum - Orthogonal Distance Regression Planes (\url{http://mathforum.org/library/drmath/view/63765.html}).

\bibitem{svd}
Singular value decomposition
(\url{https://en.wikipedia.org/wiki/Singular_value_decomposition}).

\end{thebibliography}


\end{document}
