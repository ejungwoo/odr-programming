\documentclass[a4paper,14pt]{oblivoir}

%\usepackage[margin=25mm]{geometry}
%\usepackage{url}
%\usepackage{graphicx}
%\usepackage{float}

%\documentclass[12pt]{article}
%\usepackage[hangul]{kotex}
\usepackage[margin=25mm]{geometry}
\usepackage{url}
\usepackage{amsmath}
\usepackage{relsize}

\begin{document}

\title{수직 거리 회기(Orthogonal Distance Regression (ODR))를 이용한 피팅}
\maketitle

3차원에서 주어진 N개의 점 데이터를 이용하여 가장 잘 맞는 평면을 피팅해보자.
이 문서에서 다룰 평면 피팅은 ODR(수직 거리 회기, Orthogonal Distance Regression) 피팅으로
각 점에서 평면까지 수직 거리 제곱의 합을 최소화 하는 방법이다.
일반적인 피팅 방법에서는 문제에서 최소화 할 값 $f_P$를 정한 뒤 변수를 바꿔가면서
$f_P$가 최소가 되는 점을 찾아 나가는 반복 계산을 한다.
하지만 ODR 피팅은 $f_P$가 최소가 되는 지점을 3차원 행렬의 고윳값 문제로 해결 할 수 있다는 것이 증명되어 있다.
또한 이 방법으로 평면 피팅 뿐만 아니라 선 피팅도 할 수 있다.
이 문서에서는 수직 거리 회기에 대해서 간단하게 알아보고 컴퓨터 프로그램으로 구현해 볼 것이다.

\chapter{평면 피팅}\label{chapter_fitplane}

평면($ax+by+cz=d$)과 $i$번째 점($x_i, y_i, z_i$)  사이의 수직 거리는 다음과 같다.
\begin{equation}
d_i(a,b,c,d) = \frac{\vert ax_i + by_i + cz_i + d \vert}{\sqrt{a^2 + b^2 + c^2}}.
\end{equation}
N개의 점에 대해서 거리 제곱 $d_i^2$의 합은 다음과 같다.
\begin{eqnarray} \label{f_P}
f_P(a,b,c,d) &=& \sum_i^N d_i(a,b,c,d)^2 \\\nonumber
           &=& \sum_i^N \frac{\vert ax_i + by_i + cz_i + d \vert^2}{a^2 + b^2 + c^2}.
\end{eqnarray}
$f_P$를 변수 $d$에 대해서 최소화 하면 ${\partial f_P}/{\partial d} = 0$ 이므로 다음을 얻는다.
\begin{equation} \label{minimize_d}
d = -(a\bar{x} + b\bar{y} + c\bar{z}).
\end{equation}
여기서 ($\bar{x}, \bar{y}, \bar{z}$)는 데이터의 중심점이다. 
\begin{equation} \label{centroid}
\left.
\begin{array}{ll}
\bar{x} = \sum x_i /N \\
\bar{y} = \sum y_i /N \\
\bar{z} = \sum z_i /N
\end{array}\right.
\end{equation}
식 (\ref{minimize_d})을 평면의 방정식에 대입하면 다음과 같다.
\begin{equation}
a(x_i-\bar{x}) + b(y_i-\bar{y}) + c(z_i-\bar{z}) = 0.
\end{equation}
위 식을 보면 우리가 구하려고 하는 평면은 반드시 데이터의 중심점을 지나간다는 사실을 알 수 있다.
나아가 식 (\ref{f_P})는
\begin{equation} \label{f_P2}
f_P(a,b,c) = \sum_i^N \frac{\vert aX_i + bY_i + cZ_i \vert^2}{a^2 + b^2 + c^2}
\end{equation}
이 된다.  편의상 ($x_i-\bar{x}, y_i-\bar{y}, z_i-\bar{z}$)를 ($X_i, Y_i, Z_i$)로 쓰도록 하자.
이를 행렬식으로 쓰면
\begin{eqnarray}
\mathbf{v^T} &\equiv& (a,b,c,), \\
\mathbf{M}   &\equiv& \left( \begin{array}{ccc}
  X_1 & Y_1 & Z_1 \\
  X_2 & Y_2 & Z_2 \\
  \vdots & \vdots & \vdots \\
  X_N & Y_N & Z_N 
\end{array} \right),
\end{eqnarray}
일때 다음과 같이 다시 쓸 수 있다.
\begin{equation}
f_P(\mathbf v) = \frac{\mathbf{v^T}\mathbf{M^T}\mathbf{M}\mathbf{v}}{\mathbf{v^T}\mathbf{v}}
             = \frac{\mathbf{v^T}\mathbf{A}\mathbf{v}}{\mathbf{v^T}\mathbf{v}}.
\end{equation}
\begin{equation}
(\mathbf{A}\equiv\mathbf{M^T}\mathbf{M})
\end{equation}
$f_P(\mathbf{v})$는 Rayleigh Quotient로 행렬 $\mathbf{A}$의 
가장 작은 고윳값(eigen value)을 택하였을때 최소화 되며, 반대로 가장 큰 고윳값을 택하였을때 최대화 된다.
이때 고유벡터(eigen vector) $\mathbf v$는 평면의 수직 벡터가 된다.

행렬 $\mathbf{M}$에 대해서 특이값 분해(Singular value decomposition)를 하고 이를 행렬$\mathbf{A}$에
대입하면
\begin{eqnarray}
\mathbf{M}&=&\mathbf{U}\mathbf{S}\mathbf{V^T} \\
\mathbf{A}&=&\mathbf{V}\mathbf{S^2}\mathbf{V^T}
\end{eqnarray}
가 된다. 즉 행렬 $\mathbf{A}$의 고윳값은 행렬 $\mathbf{M}$의 특이값의 제곱이 되며 
행렬 $\mathbf{A}$의 고유벡터는 행렬 $\mathbf{M}$의 특이벡터와 같다.
여기서 특이값을 구하거나 고윳값을 구하므로써 평면을 구할 수 있는데 아직 특이값에 대해 자세히 알지 못하므로
어느 방법이 더 좋은지 알 수 없다(추가 바람).
따라서 이 문서에서는 행렬 $\mathbf{A}$의 고윳값 문제로 해결하도록 하겠다.

행렬 $\mathbf{A}$를 풀어 쓰면 다음과 같다.
\begin{equation} \label{matrix_A}
\mathbf{A} = \sum_i^N\left( \begin{array}{ccc}
  X_i^2 & X_iY_i & X_iZ_i \\
  Y_iX_i & Y_i^2 & Y_iZ_i \\
  Z_iX_i & Z_iY_i & Z_i^2
\end{array} \right).
\end{equation}
이는 데이터의 개수로 나누었을때 공분산 행렬(covariance matrix)이 되는 것을 알 수 있다.

\chapter{직선 피팅}
점($x_0, y_0, z_0$)을 지나가고 ($a,b,c$)의 방향 벡터를 가지는 직선
\begin{equation}
\left.
\begin{array}{ll}
x = x_0 + at \\\nonumber
y = y_0 + bt \\\nonumber
z = z_0 + ct
\end{array}\right.
\end{equation}
과 점($x_i, y_i, z_i$) 사이의 수직 거리는 다음과 같다.
\begin{equation}
d_i(x_0,y_0,z_0,a,b,c) = \left( 
\begin{array}{ll}
  &\left(c(y_i-y_0) - b(z_i - z_0)\right)^2 \\
  + &\left(a(z_i-z_0) - c(x_i - x_0)\right)^2 \\
  + &\left(b(x_i-x_0) - a(y_i - y_0)\right)^2 
\end{array} \right)^{1/2}
\end{equation}
우리는 점과 직선사이 거리 제곱의 합을 최소화 할 것이므로 $f_L$는 다음과 같이 쓸 수 있다.
\begin{equation}
f_L(x_0,y_0,z_0,a,b,c) = \mathlarger\sum_i^N\left( 
\begin{array}{ll}
  &\left(c(y_i-y_0) - b(z_i - z_0)\right)^2 \\
  + &\left(a(z_i-z_0) - c(x_i - x_0)\right)^2 \\
  + &\left(b(x_i-x_0) - a(y_i - y_0)\right)^2 
\end{array} \right)
\end{equation}
이제 $f_L$를 $x_0$, $y_0$, $z_0$에 대해서 각각 최소화 하면
(${\partial f_L}/{\partial x_0}=0$, ${\partial f_L}/{\partial y_0}=0$, ${\partial f_L}/{\partial z_0}=0$)
다음식을 얻을 수 있다.
\begin{equation}
\frac{x_0-\bar x}{a} = \frac{y_0-\bar y}{b} = \frac{z_0-\bar z}{c}
\end{equation}
여기서 ($\bar{x}, \bar{y}, \bar{z}$)는 데이터의 중심점이며 식 (\ref{centroid})와 같다.
위 식은 ($x_0,y_0,z_0$)가 ($\bar x,\bar y,\bar z$)일때 유효하다.
따라서 우리가 구하고자 하는 직선은 데이터의 중심점을 지나간다.

데이터 점을 $R_i$, 데이터의 중심점을 $C$, 우리가 구하고자 하는 직선을 $L$, 
그 직선에 수직하고 점 $C$를 포함하는 평면을 $P$, 그리고 
$D(\alpha, \beta)$를 $\alpha$와 $\beta$ 사이의 거리라고 할때, 
피타고라스의 정리를 이용하면 다음과 같이 쓸 수 있다.
\begin{equation}
\sum_i^N D(R_i, L)^2 = \sum_i^N D(R_i, C)^2 - \sum_i^N D(R_i, P)^2
\end{equation}
위의 $\sum D(R_i, C)^2$는 상수임을 알 수 있다.
즉 우리가 최솨화 하고자 하는 $f_L(=\sum D(R_i, L)^2)$는 
$\sum D(R_i, P)^2$를 최대화 하므로써 알 수 구할 수 있다.
그런데 $\sum D(R_i, P)^2$는 \ref{chapter_fitplane}장의 $f_P$(식 (\ref{f_P}), (\ref{f_P2}))와 같으므로
행렬 $\mathbf{A}$(식 (\ref{matrix_A}))의 가장 큰 고윳값을 찾는 문제로 귀결된다.




\begin{thebibliography}{}

\bibitem{bestfitplane}
The Math Forum - Line of Best Fit For Points in Three Dimensional Space
(\url{http://mathforum.org/library/drmath/view/69103.html}).

\bibitem{bestfitline}
The Math Forum - Orthogonal Distance Regression Planes (\url{http://mathforum.org/library/drmath/view/63765.html}).

\bibitem{svd}
Singular value decomposition
(\url{https://en.wikipedia.org/wiki/Singular_value_decomposition}).

\end{thebibliography}


\end{document}
